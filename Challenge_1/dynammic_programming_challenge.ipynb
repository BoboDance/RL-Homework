{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import quanser_robots\n",
    "\n",
    "import sys\n",
    "#sys.load('../')\n",
    "sys.path.insert(0,'../')\n",
    "from Challenge_1.Algorithms.PolicyIteration import PolicyIteration\n",
    "from Challenge_1.Algorithms.ValueIteration import ValueIteration\n",
    "from Challenge_1.Models.NNModelPendulum import NNModelPendulum\n",
    "from Challenge_1.Models.NNModelQube import NNModelQube\n",
    "from Challenge_1.Models.SklearnModel import SklearnModel\n",
    "from Challenge_1.util.ColorLogger import enable_color_logging\n",
    "from Challenge_1.util.DataGenerator import DataGenerator\n",
    "from Challenge_1.util.Discretizer import Discretizer\n",
    "import itertools\n",
    "from torch.optim.lr_scheduler import *\n",
    "enable_color_logging(debug_lvl=logging.INFO)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "seed = 1234\n",
    "# avoid auto removal of import with pycharm\n",
    "quanser_robots\n",
    "\n",
    "env_name = \"Pendulum-v2\"\n",
    "#env_name = \"Qube-v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "n_steps = 500 #10000\n",
    "batch_size = 512\n",
    "lr = 1e-3\n",
    "path = \"./NN-state_dict\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the gym-environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create both neural net models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if env_name == 'Pendulum-v2':\n",
    "    dynamics_model = NNModelPendulum(n_inputs=env.observation_space.shape[0] + env.action_space.shape[0] + nb_angle_features,\n",
    "                             n_outputs=env.observation_space.shape[0],\n",
    "                             scaling=env.observation_space.high, optimizer='adam')\n",
    "\n",
    "    reward_model = NNModelPendulum(n_inputs=env.observation_space.shape[0] + env.action_space.shape[0] + nb_angle_features,\n",
    "                           n_outputs=1,\n",
    "                           scaling=None, optimizer='adam')\n",
    "elif env_name == 'Qube-v0':\n",
    "    dynamics_model = NNModelQube(n_inputs=env.observation_space.shape[0] + env.action_space.shape[0] + nb_angle_features,\n",
    "                         n_outputs=env.observation_space.shape[0],\n",
    "                         scaling=env.observation_space.high, optimizer='adam')\n",
    "\n",
    "    reward_model = NNModelQube(n_inputs=env.observation_space.shape[0] + env.action_space.shape[0] + nb_angle_features,\n",
    "                           n_outputs=1,\n",
    "                           scaling=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfunction = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(env_name, seed, n_samples):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    dg_train = DataGenerator(env_name=env_name, seed=seed)\n",
    "\n",
    "    # s_prime - future state after you taken the action from state s\n",
    "    state_prime, state, action, reward = dg_train.get_samples(n_samples)\n",
    "\n",
    "    nb_angle_features = 1\n",
    "    \n",
    "    # replace the angle feature by 2 new features which are the sin(angle) and cos(angle)\n",
    "    state_enhanced = np.zeros((len(state), state.shape[1]+nb_angle_features))\n",
    "    state_enhanced[:, 0] = np.cos(state[:, 0])\n",
    "    state_enhanced[:, 1] = np.sin(state[:, 0])\n",
    "    state_enhanced[:, 2] = state[:, 1]\n",
    "\n",
    "    # create training input pairs\n",
    "    s_a_pairs = np.concatenate([state_enhanced, action[:, np.newaxis]], axis=1).reshape(-1, state_enhanced.shape[1] +\n",
    "                                                                               env.action_space.shape[0])\n",
    "    reward = reward.reshape(-1, 1)\n",
    "\n",
    "    return s_a_pairs, state_prime, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_train = DataGenerator(env_name=env_name, seed=seed)\n",
    "\n",
    "# s_prime - future state after you taken the action from state s\n",
    "state_prime, state, action, reward = dg_train.get_samples(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_angle_features = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the angle feature by 2 new features which are the sin(angle) and cos(angle)\n",
    "state_enhanced = np.zeros((len(state), state.shape[1]+nb_angle_features))\n",
    "state_enhanced[:, 0] = np.cos(state[:, 0])\n",
    "state_enhanced[:, 1] = np.sin(state[:, 0])\n",
    "state_enhanced[:, 2] = state[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training input pairs\n",
    "s_a_pairs = np.concatenate([state_enhanced, action[:, np.newaxis]], axis=1).reshape(-1, state_enhanced.shape[1] +\n",
    "                                                                           env.action_space.shape[0])\n",
    "reward = reward.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_a_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_a_pairs_train, state_prime_train, reward_train = create_dataset(env_name, seed, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test input pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_test = DataGenerator(env_name=env_name, seed=seed + 1)\n",
    "s_prime, s, a, r = dg_test.get_samples(n_samples)\n",
    "    \n",
    "s_a = np.concatenate([s, a[:, np.newaxis]], axis=1).reshape(-1, env.observation_space.shape[0] +\n",
    "                                                            env.action_space.shape[0])\n",
    "r = r.reshape(-1, 1)\n",
    "s_prime = s_prime.reshape(-1, env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_a_pairs_test, state_prime_test, reward_test = create_dataset(env_name, seed+1, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_low = np.concatenate([env.observation_space.low, env.action_space.low])\n",
    "X_high = np.concatenate([env.observation_space.high, env.action_space.high])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_low = np.min(s_a_pairs_train, axis=0)\n",
    "X_high = np.max(s_a_pairs_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_input(x):\n",
    "    \"\"\"\n",
    "    Normalizes the input data by using linear scaling to [0,1]\n",
    "    \"\"\"\n",
    "    x = x - X_low\n",
    "    x /= (X_high - X_low)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the input X for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_a_pairs_train = normalize_input(s_a_pairs_train)\n",
    "s_a_pairs_test = normalize_input(s_a_pairs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(s_a_pairs).float()\n",
    "Y = torch.from_numpy(state_prime).float()\n",
    "\n",
    "X_val = torch.from_numpy(s_a)\n",
    "Y_val = s_prime\n",
    "\n",
    "#X -= 0.5\n",
    "#Y = (Y - Y.min())\n",
    "#Y /= Y.max()\n",
    "#Y -= 0.5\n",
    "\n",
    "model = dynamics_model\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4) #, weight_decay=1e12)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=True)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, lr, eta_min=0, last_epoch=-1)\n",
    "#scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "#scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, X, y):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # y = torch.from_numpy(y).float()\n",
    "        #X = torch.from_numpy(X).float()\n",
    "\n",
    "        out = model(X)\n",
    "\n",
    "        mse_test = ((out.detach().numpy() - y) ** 2).mean(axis=0)\n",
    "\n",
    "        print(\"Test MSE: {}\".format(mse_test))\n",
    "    return mse_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, X, Y, X_val, Y_val, n_epochs=150, batch_size=32):\n",
    "    \n",
    "    X = torch.from_numpy(X).float()\n",
    "    Y = torch.from_numpy(Y).float()\n",
    "\n",
    "    X_val = torch.from_numpy(X_val)\n",
    "    Y_val = Y_val\n",
    "\n",
    "    # https://stackoverflow.com/questions/45113245/how-to-get-mini-batches-in-pytorch-in-a-clean-and-efficient-way\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # X is a torch Variable\n",
    "        permutation = torch.randperm(X.size()[0])\n",
    "\n",
    "        for i in range(0,X.size()[0], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = X[indices], Y[indices]\n",
    "\n",
    "            # in case you wanted a semi-full example\n",
    "            outputs = model.forward(batch_x)\n",
    "            loss = lossfunction(outputs,batch_y)\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 25 == 0:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] /= 2\n",
    "\n",
    "        print(\"Step: {:d} -- total loss: {:3.8f}\".format(epoch, loss.item()))\n",
    "        val_loss.append(validate_model(model, X_val, Y_val))\n",
    "        \n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_no_norm, val_loss_no_norm = train(dynamics_model, optimizer=optimizer,\n",
    "                             X=s_a_pairs, Y=state_prime, X_val=s_a, Y_val=s_prime, n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = train(dynamics_model, optimizer=optimizer,\n",
    "                             X=s_a_pairs, Y=state_prime, X_val=s_a, Y_val=s_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_norm_sincos, val_loss_norm_sincos = train(dynamics_model, optimizer=optimizer,\n",
    "                             X=s_a_pairs_train, Y=state_prime_train, X_val=s_a_pairs_test, Y_val=state_prime_test, n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Qube-v0: Learning Dynamics\\n Batch-Size=32, lr=1e-3, optimizer=Adam')\n",
    "#plt.plot(train_loss, label='train_loss')\n",
    "plt.plot(val_loss, label='val-loss with normalization')\n",
    "plt.plot(val_loss_no_norm, label='val-loss no normalization')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.savefig('Qube_Dynamics_normalization_comparision.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Qube-v0: Learning Dynamics\\n Batch-Size=32, lr=1e-3, optimizer=Adam')\n",
    "#plt.plot(train_loss, label='train_loss')\n",
    "plt.plot(val_loss, label='val-loss with normalization\\nTest MSE: [0.009659 0.325742 0.617882 0.561754]')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.savefig('Qube_Dynamics_normalization.png', bbox='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = train(dynamics_model, optimizer=optimizer,\n",
    "                             X=s_a_pairs, Y=state_prime, X_val=s_a, Y_val=s_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dynamics_model.state_dict(), \"./NN-state_dict_dynamics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model.train_network(s_a_pairs, reward, n_steps, path + \"_reward\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = train(reward_model, optimizer=optimizer,\n",
    "                             X=s_a_pairs, Y=reward, X_val=s_a, Y_val=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(reward_model.state_dict(), \"./NN-state_dict_reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_policy_iteration(env_name, algorithm=\"vi\", n_samples=10000, bins_state=50, bins_action=2, seed=1,\n",
    "                           theta=1e-9, use_MC=True, MC_samples=500, dense_location=[\"center\", \"edge\"]):\n",
    "    env = gym.make(env_name)\n",
    "    print(\"Training with {} samples.\".format(n_samples))\n",
    "\n",
    "    dg_train = DataGenerator(env_name=env_name, seed=seed)\n",
    "\n",
    "    # s_prime - future state after you taken the action from state s\n",
    "    state_prime, state, action, reward = dg_train.get_samples(n_samples)\n",
    "\n",
    "    # create training input pairs\n",
    "    s_a_pairs = np.concatenate([state, action[:, np.newaxis]], axis=1)\n",
    "\n",
    "    # solve regression problem s_prime = f(s,a)\n",
    "    # dynamics_model = SklearnModel(type=\"rf\")\n",
    "    # dynamics_model.fit(s_a_pairs, state_prime)\n",
    "    #\n",
    "    # # solve regression problem r = g(s,a)\n",
    "    # reward_model = SklearnModel(type=\"rf\")\n",
    "    # reward_model.fit(s_a_pairs, reward)\n",
    "\n",
    "    # But performance should not change much\n",
    "    dynamics_model = NNModel(n_inputs=env.observation_space.shape[0] + env.action_space.shape[0],\n",
    "                             n_outputs=env.observation_space.shape[0],\n",
    "                             scaling=env.observation_space.high)\n",
    "\n",
    "    reward_model = NNModel(n_inputs=env.observation_space.shape[0] + env.action_space.shape[0],\n",
    "                           n_outputs=1,\n",
    "                           scaling=None)\n",
    "\n",
    "    dynamics_model.load_model(\"./NN-state_dict_dynamics\") #_10000_200hidden\")\n",
    "    reward_model.load_model(\"./NN-state_dict_reward\") #_10000_200hidden\")\n",
    "\n",
    "    # center, edge for pendulum is best for RF\n",
    "    # edge, center is best for NN\n",
    "    discretizer_state = Discretizer(n_bins=bins_state, space=env.observation_space,\n",
    "                                    dense_locations=dense_location)\n",
    "    discretizer_action = Discretizer(n_bins=bins_action, space=env.action_space)\n",
    "\n",
    "    if algorithm == \"pi\":\n",
    "        algo = PolicyIteration(env=env, dynamics_model=dynamics_model, reward_model=reward_model,\n",
    "                               discretizer_state=discretizer_state, discretizer_action=discretizer_action, theta=theta)\n",
    "    elif algorithm == \"vi\":\n",
    "        algo = ValueIteration(env=env, dynamics_model=dynamics_model, reward_model=reward_model,\n",
    "                              discretizer_state=discretizer_state, discretizer_action=discretizer_action, theta=theta,\n",
    "                              use_MC=use_MC, MC_samples=MC_samples)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    algo.run(max_iter=100)\n",
    "\n",
    "    return algo.policy, discretizer_action, discretizer_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, discretizer_action, discretizer_state = start_policy_iteration(env_name, seed=seed, n_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss[60:])\n",
    "plt.plot(val_loss[60:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss[60:])\n",
    "plt.plot(val_loss[60:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
