{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "\n",
    "* In this Jupyter Notebook both the dynamic models and reward models are trained and\n",
    "later exported as a paramter dictionary for later usage in with pytorch\n",
    "* We use a neural network for both dynamics and reward\n",
    "* You can define one of the two environemnts in the `Settings` block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import quanser_robots\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "from Challenge_1.Algorithms.PolicyIteration import PolicyIteration\n",
    "from Challenge_1.Algorithms.ValueIteration import ValueIteration\n",
    "from Challenge_1.Models.NNModelPendulum import NNModelPendulum\n",
    "from Challenge_1.Models.NNModelQube import NNModelQube\n",
    "from Challenge_1.Models.SklearnModel import SklearnModel\n",
    "from Challenge_1.util.ColorLogger import enable_color_logging\n",
    "from Challenge_1.util.DataGenerator import DataGenerator\n",
    "from Challenge_1.util.Discretizer import Discretizer\n",
    "from Challenge_1.util.state_preprocessing import reconvert_state_to_angle, normalize_input, get_feature_space_boundaries, convert_state_to_sin_cos\n",
    "import itertools\n",
    "from torch.optim.lr_scheduler import *\n",
    "enable_color_logging(debug_lvl=logging.INFO)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "seed = 1234\n",
    "# avoid auto removal of import with pycharm\n",
    "quanser_robots\n",
    "\n",
    "env_name = \"Pendulum-v2\"\n",
    "#env_name = \"Qube-v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "n_steps = 500 #10000\n",
    "batch_size_dynamics = 64\n",
    "batch_size_reward = 256\n",
    "lr = 1e-3\n",
    "path = \"./NN-state_dict\"\n",
    "optimizer = 'rmsprop'\n",
    "export_plots = True\n",
    "\n",
    "# index list of angle features\n",
    "if env_name == 'Pendulum-v2':\n",
    "    angle_features = [0]\n",
    "elif env_name == \"Qube-v0\":\n",
    "    angle_features = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the gym-environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create both neural net models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_low, X_high = get_feature_space_boundaries(env, angle_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling defines how our outputs will be scaled after the tanh function\n",
    "# for this we use all state features ergo all of X_high excluding the last action feature\n",
    "scaling = X_high[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = env.observation_space.shape[0] + env.action_space.shape[0] + len(angle_features)\n",
    "n_outputs = env.observation_space.shape[0] + len(angle_features)\n",
    "if env_name == 'Pendulum-v2':\n",
    "    dynamics_model = NNModelPendulum(n_inputs=n_inputs,\n",
    "                             n_outputs=n_outputs,\n",
    "                             scaling=scaling, optimizer='adam')\n",
    "\n",
    "    reward_model = NNModelPendulum(n_inputs=n_inputs,\n",
    "                           n_outputs=1,\n",
    "                           scaling=None, optimizer='adam')\n",
    "elif env_name == 'Qube-v0':\n",
    "    dynamics_model = NNModelQube(n_inputs=n_inputs,\n",
    "                         n_outputs=n_outputs,\n",
    "                         scaling=scaling, optimizer='adam')\n",
    "\n",
    "    reward_model = NNModelQube(n_inputs=n_inputs,\n",
    "                           n_outputs=1,\n",
    "                           scaling=None, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfunction = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(env_name, seed, n_samples):\n",
    "    \"\"\"\n",
    "    Creates the dataset for training the NN\n",
    "    \"\"\"\n",
    "    \n",
    "    dg_train = DataGenerator(env_name=env_name, seed=seed)\n",
    "\n",
    "    # s_prime - future state after you taken the action from state s\n",
    "    state_prime, state, action, reward = dg_train.get_samples(n_samples)\n",
    "\n",
    "    state_sincos = convert_state_to_sin_cos(state, angle_features)\n",
    "    state_prime = convert_state_to_sin_cos(state_prime, angle_features)\n",
    "    \n",
    "    # create training input pairs\n",
    "    s_a_pairs = np.concatenate([state_sincos, action[:, np.newaxis]], axis=1).reshape(-1, state_sincos.shape[1] +\n",
    "                                                                               env.action_space.shape[0])\n",
    "    reward = reward.reshape(-1, 1)\n",
    "\n",
    "    return s_a_pairs, state_prime, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_a_pairs_train, state_prime_train, reward_train = create_dataset(env_name, seed, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test input pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_a_pairs_test, state_prime_test, reward_test = create_dataset(env_name, seed+1, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the input X for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_a_pairs_train = normalize_input(s_a_pairs_train, X_low, X_high)\n",
    "s_a_pairs_test = normalize_input(s_a_pairs_test, X_low, X_high)\n",
    "\n",
    "state_prime_train = normalize_input(state_prime_train, X_low[:-1], X_high[:-1])\n",
    "state_prime_test = normalize_input(state_prime_test, X_low[:-1], X_high[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if optimizer == 'rmsprop':\n",
    "    optimizer_dynamics = optim.RMSprop(dynamics_model.parameters(), lr=lr)\n",
    "    optimizer_reward = optim.RMSprop(reward_model.parameters(), lr=lr)\n",
    "elif optimizer == 'adam':\n",
    "    optimizer_dynamics = optim.Adam(dynamics_model.parameters(), lr=lr)\n",
    "    optimizer_reward = optim.Adam(reward_model.parameters(), lr=lr)\n",
    "elif optimizer == 'sgd':\n",
    "    optimizer_dynamics = optim.SGD(dynamics_model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "    optimizer_reward = optim.SGD(reward_model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "else:\n",
    "    raise Exception('Unsupported optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, X, y):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        out = model(X)\n",
    "\n",
    "        mse_test = ((out.detach().numpy() - y) ** 2).mean(axis=0)\n",
    "\n",
    "        print(\"Test MSE: {}\".format(mse_test))\n",
    "        print(\"Test MSE (mean): {}\".format(mse_test.mean()))\n",
    "\n",
    "    return mse_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, X, Y, X_val, Y_val, n_epochs=150, batch_size=32):\n",
    "    \n",
    "    X = torch.from_numpy(X).float()\n",
    "    Y = torch.from_numpy(Y).float()\n",
    "\n",
    "    X_val = torch.from_numpy(X_val)\n",
    "    Y_val = Y_val\n",
    "\n",
    "    # https://stackoverflow.com/questions/45113245/how-to-get-mini-batches-in-pytorch-in-a-clean-and-efficient-way\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # X is a torch Variable\n",
    "        permutation = torch.randperm(X.size()[0])\n",
    "\n",
    "        for i in range(0,X.size()[0], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = X[indices], Y[indices]\n",
    "\n",
    "            # in case you wanted a semi-full example\n",
    "            outputs = model.forward(batch_x)\n",
    "            loss = lossfunction(outputs,batch_y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] /= 2\n",
    "\n",
    "        print(\"Epoch: {:d} -- total loss: {:3.8f}\".format(epoch+1, loss.item()))\n",
    "        train_loss.append(loss.item())\n",
    "        val_loss.append(validate_model(model, X_val, Y_val))\n",
    "\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Dynamics Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_dynamics, val_loss_dynamics = train(dynamics_model, optimizer=optimizer_dynamics,\n",
    "                             X=s_a_pairs_train, Y=state_prime_train, X_val=s_a_pairs_test, Y_val=state_prime_test,batch_size=batch_size_dynamics, n_epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('%s: Learning Dynamics\\n Batch-Size=%d, lr=%f, optimizer=%s' %\n",
    "          (env_name, batch_size_dynamics, lr, optimizer))\n",
    "#plt.plot(train_loss, label='train_loss')\n",
    "plt.plot(train_loss_dynamics, label='train-loss')\n",
    "plt.plot(val_loss_dynamics, label='val-loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "if export_plots is True:\n",
    "    plt.savefig('Plots/%s_Dynamics.png' % env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained weights for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_name = \"./Weights/model_dynamics_%s_mse_%.8f.params\" % (env_name, val_loss_dynamics[-1])\n",
    "torch.save(dynamics_model.state_dict(), export_name)\n",
    "print('Your weights have been saved to %s successfully!' % export_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_reward, val_loss_reward = train(reward_model, optimizer=optimizer_reward,\n",
    "                             X=s_a_pairs_train, Y=reward_train, X_val=s_a_pairs_test, Y_val=reward_test, batch_size=batch_size_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('%s: Learning Rewards\\n Batch-Size=%d, lr=%f, optimizer=%s' %\n",
    "          (env_name, batch_size_dynamics, lr, optimizer))\n",
    "plt.plot(train_loss_reward, label='train-loss')\n",
    "plt.plot(val_loss_reward, label='val-loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "if export_plots is True:\n",
    "    plt.savefig('Plots/%s_Reward.png' % env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the weights of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_name = \"./Weights/model_reward_%s_mse_%.8f.params\" % (env_name, val_loss_reward[-1])\n",
    "torch.save(reward_model.state_dict(), export_name)\n",
    "print('Your weights have been saved to %s successfully!' % export_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
