"""
Submission template for Programming Challenge 3: Policy Gradient Methods.


Fill in submission info and implement 6 functions:

- load_reinforce_policy
- train_reinforce_policy
- load_npg_policy
- train_npg_policy
- load_nes_policy
- train_nes_policy

Keep Monitor files generated by Gym while learning within your submission.
Example project structure:

challenge3_submission/
  - challenge3_submission.py
  - reinforce.py
  - npg.py
  - nes.py
  - reinforce_eval/
  - npg_eval/
  - nes_eval/
  - reinforce_train/
  - npg_train/
  - nes_train/
  - supplementary/

Directories `npg_eval/`, `npg_train/`, etc. are autogenerated by Gym (see below).
Put all additional results into the `supplementary` directory.

Performance of the policies returned by `load_xxx_policy` functions
will be evaluated and used to determine the winner of the challenge.
Learning progress and learning algorithms will be checked to confirm
correctness and fairness of implementation. Supplementary material
will be manually analyzed to identify outstanding submissions.
"""
import torch

import gym
import numpy as np

from Challenge_3.NPG.NaturalPG import NaturalPG
from Challenge_3.Policy.ContinuousPolicy import ContinuousPolicy
from Challenge_3.Policy.DiscretePolicy import DiscretePolicy
from Challenge_3.REINFORCE.reinforce import REINFORCE
from Challenge_3.Util import set_seed, make_env_step_silent

info = dict(
    group_number=16,  # change if you are an existing seminar/project group
    authors="Jannis Weil; Johannes Czech; Fabian Otto",
    description="Explain what your code does and how. "
                "Keep this description short "
                "as it is not meant to be a replacement for docstrings "
                "but rather a quick summary to help the grader.")


def load_reinforce_policy():
    """
    Load pretrained REINFORCE policy from file.

    The policy must return a continuous action `a`
    that can be directly passed to `Levitation-v1` env.

    :return: function pi: s -> a
    """
    env = gym.make("Levitation-v1")
    discrete_actions = np.linspace(env.action_space.low, env.action_space.high, 2)
    reinforce_model = DiscretePolicy(env, discrete_actions, n_hidden_units=8)
    reinforce_model.load_state_dict(torch.load("./checkpoints/reinforce_submission.pth"))

    def policy_fun(obs):
        action, _ = reinforce_model.choose_action_by_sampling(obs)
        return np.array(action)

    return policy_fun


def train_reinforce_policy(env):
    """
    Execute your implementation of the REINFORCE learning algorithm.

    This function should start your code placed in a separate file.

    :param env: gym.Env
    :return: function pi: s -> a
    """
    set_seed(env, 42)

    discrete_actions = np.linspace(env.action_space.low, env.action_space.high, 2)
    reinforce_model = DiscretePolicy(env, discrete_actions, n_hidden_units=8)

    low = env.observation_space.low
    low[1] = 0

    reinforce = REINFORCE(env, reinforce_model, gamma=1, lr=0.1, normalize_observations=False, low=low)
    reinforce.train(min_steps=100, save_best=True, render_episodes_mod=100, max_episodes=10, save_path="./checkpoints/reinforce_train.pth")

    # load the best model again
    reinforce_model.load_state_dict(torch.load("./checkpoints/reinforce_train.pth"))

    def policy_fun(obs):
        action, _ = reinforce_model.choose_action_by_sampling(obs)
        return np.array(action)

    return policy_fun


def load_npg_policy():
    """
    Load pretrained NPG policy from file.

    The policy must return a continuous action `a`
    that can be directly passed to `BallBalancerSim-v0` env.

    :return: function pi: s -> a
    """
    env = gym.make("BallBalancerSim-v0")
    actor = ContinuousPolicy(env, n_hidden_units=32, state_dependent_sigma=False)
    actor.load_state_dict(torch.load("./checkpoints/npg_submission.pth"))

    def policy_fun(obs):
        action, _ = actor.choose_action_by_sampling(obs)
        return np.array(action)

    return policy_fun


def train_npg_policy(env):
    """
    Execute your implementation of the NPG learning algorithm.

    This function should start your code placed in a separate file.

    :param env: gym.Env
    :return: function pi: s -> a
    """
    set_seed(env, 1)

    actor = ContinuousPolicy(env, n_hidden_units=32, state_dependent_sigma=False)

    naturalPG = NaturalPG(env, actor, gamma=0.99, use_tensorboard=True)
    naturalPG.train(min_steps=5000, max_episodes=4500, step_size=0.25, save_path="./checkpoints/npg_train.pth")

    # load the best model again
    actor.load_state_dict(torch.load("./checkpoints/npg_train.pth"))

    def policy_fun(obs):
        action, _ = actor.choose_action_by_sampling(obs)
        return action

    return policy_fun


def load_nes_policy():
    """
    Load pretrained NES policy from file.

    The policy must return a continuous action `a`
    that can be directly passed to `BallBalancerSim-v0` env.

    :return: function pi: s -> a
    """
    return lambda obs: np.array([299792458.0])


def train_nes_policy(env):
    """
    Execute your implementation of the NES learning algorithm.

    This function should start your code placed in a separate file.

    :param env: gym.Env
    :return: function pi: s -> a
    """
    return lambda obs: np.array([9.10938356 * 1e-31])


# ==== Example evaluation
def main():
    import gym
    from gym.wrappers.monitor import Monitor
    import quanser_robots

    def evaluate(env, policy, num_evlas=25):
        ep_returns = []
        for eval_num in range(num_evlas):
            episode_return = 0
            dones = False
            obs = env.reset()
            while not dones:
                action = policy(obs)
                obs, rewards, dones, info = env.step(action)
                episode_return += rewards
            ep_returns.append(episode_return)
        return ep_returns

    def render(env, policy):
        obs = env.reset()
        done = False
        while not done:
            env.render()
            act = policy(obs)
            obs, _, done, _ = env.step(act)

    def check(env, policy):
        # CHANGED: Do not render levitation!
        if env.unwrapped.spec.id != 'Levitation-v1':
            render(env, policy)
        ret_all = evaluate(env, policy)
        print(np.mean(ret_all), np.std(ret_all))
        env.close()

    print("REINFORCE")

    # REINFORCE I: Check learned policy
    env = Monitor(gym.make('Levitation-v1'), 'reinforce_eval', force=True)
    #policy = load_reinforce_policy()
    #check(env, policy)

    # REINFORCE II: Check learning procedure
    env = Monitor(gym.make('Levitation-v1'), 'reinforce_train', video_callable=False, force=True)
    #policy = train_reinforce_policy(env)
    #check(env, policy)

    print("NPG")

    # NPG I: Check learned policy
    env = Monitor(gym.make('BallBalancerSim-v0'), 'npg_eval', force=True)
    policy = load_npg_policy()
    check(env, policy)

    # NPG II: Check learning procedure
    env = Monitor(gym.make('BallBalancerSim-v0'), 'npg_train', video_callable=False, force=True)
    policy = train_npg_policy(env)
    check(env, policy)

    print("NES")

    # NES I: Check learned policy
    env = Monitor(gym.make('BallBalancerSim-v0'), 'nes_eval', force=True)
    policy = load_nes_policy()
    check(env, policy)

    # NES II: Check learning procedure
    env = Monitor(gym.make('BallBalancerSim-v0'), 'nes_train', video_callable=False, force=True)
    policy = train_nes_policy(env)
    check(env, policy)


if __name__ == '__main__':
    main()
