"""
Submission template for Programming Challenge 3: Policy Gradient Methods.


Fill in submission info and implement 6 functions:

- load_reinforce_policy
- train_reinforce_policy
- load_npg_policy
- train_npg_policy
- load_nes_policy
- train_nes_policy

Keep Monitor files generated by Gym while learning within your submission.
Example project structure:

challenge3_submission/
  - challenge3_submission.py
  - reinforce.py
  - npg.py
  - nes.py
  - reinforce_eval/
  - npg_eval/
  - nes_eval/
  - reinforce_train/
  - npg_train/
  - nes_train/
  - supplementary/

Directories `npg_eval/`, `npg_train/`, etc. are autogenerated by Gym (see below).
Put all additional results into the `supplementary` directory.

Performance of the policies returned by `load_xxx_policy` functions
will be evaluated and used to determine the winner of the challenge.
Learning progress and learning algorithms will be checked to confirm
correctness and fairness of implementation. Supplementary material
will be manually analyzed to identify outstanding submissions.
"""
import pickle
from functools import partial

import torch

import gym
import numpy as np

from Challenge_3.NES.nes import NES
from Challenge_3.NPG.NaturalPG import NaturalPG
from Challenge_3.Policy.ContinuousPolicy import ContinuousPolicy
from Challenge_3.Policy.DiscretePolicy import DiscretePolicy
from Challenge_3.Policy.NESPolicy import NESPolicy
from Challenge_3.REINFORCE.reinforce import REINFORCE
from Challenge_3.Util import set_seed, make_env_step_silent, nes_load_model_weights, get_reward

info = dict(
    group_number=16,  # change if you are an existing seminar/project group
    authors="Jannis Weil; Johannes Czech; Fabian Otto",
    description="Our code covers an implementation for all requested three algorithms:"
                "REINFORCE, Natural Evolution Strategies (NES), Natural Policy Gradient (NPG)"
                "which have been trained and evaluated on "
                "Levitation-v1, BallBalancerSim-v0, BallBalancerSim-v0 respectively."
                "Our REINFORCE consists of a discrete as well as a continuous version."
                "Our chosen policy uses the discrete version."
                "For more detailed documentation please take a look at our README.md.")


def load_reinforce_policy():
    """
    Load pretrained REINFORCE policy from file.

    The policy must return a continuous action `a`
    that can be directly passed to `Levitation-v1` env.

    :return: function pi: s -> a
    """
    env = gym.make("Levitation-v1")
    discrete_actions = np.linspace(env.action_space.low, env.action_space.high, 2)
    reinforce_model = DiscretePolicy(env, discrete_actions, n_hidden_units=8)
    reinforce_model.load_state_dict(torch.load("./checkpoints/reinforce_submission.pth"))

    def policy_fun(obs):
        action, _ = reinforce_model.choose_action_by_sampling(obs)
        return np.array(action)

    return policy_fun


def train_reinforce_policy(env):
    """
    Execute your implementation of the REINFORCE learning algorithm.

    This function should start your code placed in a separate file.

    :param env: gym.Env
    :return: function pi: s -> a
    """
    set_seed(env, 42)

    discrete_actions = np.linspace(env.action_space.low, env.action_space.high, 2)
    reinforce_model = DiscretePolicy(env, discrete_actions, n_hidden_units=8)

    low = env.observation_space.low
    low[1] = 0

    reinforce = REINFORCE(env, reinforce_model, gamma=1, lr=0.1, normalize_observations=False, low=low)
    reinforce.train(min_steps=100, save_best=True, render_episodes_mod=100, max_episodes=10,
                    save_path="./checkpoints/reinforce_train.pth")

    # load the best model again
    reinforce_model.load_state_dict(torch.load("./checkpoints/reinforce_train.pth"))

    def policy_fun(obs):
        action, _ = reinforce_model.choose_action_by_sampling(obs)
        return np.array(action)

    return policy_fun


def load_npg_policy():
    """
    Load pretrained NPG policy from file.

    The policy must return a continuous action `a`
    that can be directly passed to `BallBalancerSim-v0` env.

    :return: function pi: s -> a
    """
    env = gym.make("BallBalancerSim-v0")
    actor = ContinuousPolicy(env, n_hidden_units=32, state_dependent_sigma=False)
    actor.load_state_dict(torch.load("./checkpoints/npg_submission.pth"))

    def policy_fun(obs):
        action, _ = actor.choose_action_by_sampling(obs)
        return np.array(action)

    return policy_fun


def train_npg_policy(env):
    """
    Execute your implementation of the NPG learning algorithm.

    This function should start your code placed in a separate file.

    :param env: gym.Env
    :return: function pi: s -> a
    """
    set_seed(env, 1)

    actor = ContinuousPolicy(env, n_hidden_units=32, state_dependent_sigma=False)

    naturalPG = NaturalPG(env, actor, gamma=0.99, use_tensorboard=True)
    naturalPG.train(min_steps=5000, max_episodes=4500, step_size=0.25, save_path="./checkpoints/npg_train.pth")

    # load the best model again
    actor.load_state_dict(torch.load("./checkpoints/npg_train.pth"))

    def policy_fun(obs):
        action, _ = actor.choose_action_by_sampling(obs)
        return action

    return policy_fun


def load_nes_policy():
    """
    Load pretrained NES policy from file.

    The policy must return a continuous action `a`
    that can be directly passed to `BallBalancerSim-v0` env.

    :return: function pi: s -> a
    """

    env = gym.make('BallBalancerSim-v0')
    model = NESPolicy(env, n_hidden_units=10)
    weights = pickle.load(open(f"./checkpoints/nes_submission.pkl", 'rb'))
    nes_load_model_weights(weights, model)

    def policy_fun(obs):
        with torch.no_grad():
            return model(torch.Tensor(obs)).detach().numpy()

    return policy_fun


def train_nes_policy(env):
    """
    Execute your implementation of the NES learning algorithm.

    This function should start your code placed in a separate file.

    :param env: gym.Env
    :return: function pi: s -> a
    """

    set_seed(env, 42)

    # important parameters
    population_size = 20
    sigma = 1
    lr = 5e-2

    # early stopping if goal is reached for n steps
    reward_goal = 700
    consecutive_goal_stopping = 20

    thread_count = 1
    render = False
    iterations = 1000
    normalize_rewards = True

    # decay for lr and exploration
    decay = 1
    sigma_decay = 1

    model = NESPolicy(env, n_hidden_units=10)

    partial_func = partial(get_reward, model=model, env=env)
    global_parameters = list(model.parameters())

    nes = NES(global_parameters, partial_func, population_size=population_size, sigma=sigma, learning_rate=lr,
              reward_goal=reward_goal, consecutive_goal_stopping=consecutive_goal_stopping, threadcount=thread_count,
              render_test=render, decay=decay, sigma_decay=sigma_decay, normalize_reward=normalize_rewards,
              save_path="./checkpoints/nes_train.pkl")

    nes.run(iterations=iterations, print_mod=1)

    weights = pickle.load(open(f"./checkpoints/nes_train.pkl", 'rb'))
    nes_load_model_weights(weights, model)

    def policy_fun(obs):
        with torch.no_grad():
            return model(torch.Tensor(obs)).detach().numpy()

    return policy_fun


# ==== Example evaluation
def main():
    import gym
    from gym.wrappers.monitor import Monitor
    import quanser_robots

    def evaluate(env, policy, num_evlas=25):
        ep_returns = []
        for eval_num in range(num_evlas):
            episode_return = 0
            dones = False
            obs = env.reset()
            while not dones:
                action = policy(obs)
                obs, rewards, dones, info = env.step(action)
                episode_return += rewards
            ep_returns.append(episode_return)
        return ep_returns

    def render(env, policy):
        obs = env.reset()
        done = False
        while not done:
            env.render()
            act = policy(obs)
            obs, _, done, _ = env.step(act)

    def check(env, policy):
        # CHANGED: Do not render levitation!
        if env.unwrapped.spec.id != 'Levitation-v1':
            render(env, policy)
        ret_all = evaluate(env, policy)
        print(np.mean(ret_all), np.std(ret_all))
        env.close()

    print("REINFORCE")

    # REINFORCE I: Check learned policy
    env = Monitor(gym.make('Levitation-v1'), 'reinforce_eval', force=True)
    policy = load_reinforce_policy()
    check(env, policy)

    # REINFORCE II: Check learning procedure
    env = Monitor(gym.make('Levitation-v1'), 'reinforce_train', video_callable=False, force=True)
    policy = train_reinforce_policy(env)
    check(env, policy)

    print("NPG")

    # NPG I: Check learned policy
    env = Monitor(gym.make('BallBalancerSim-v0'), 'npg_eval', force=True)
    policy = load_npg_policy()
    check(env, policy)

    # NPG II: Check learning procedure
    env = Monitor(gym.make('BallBalancerSim-v0'), 'npg_train', video_callable=False, force=True)
    policy = train_npg_policy(env)
    check(env, policy)

    print("NES")

    # NES I: Check learned policy
    env = Monitor(gym.make('BallBalancerSim-v0'), 'nes_eval', force=True)
    policy = load_nes_policy()
    check(env, policy)

    # NES II: Check learning procedure
    env = Monitor(gym.make('BallBalancerSim-v0'), 'nes_train', video_callable=False, force=True)
    policy = train_nes_policy(env)
    check(env, policy)


if __name__ == '__main__':
    main()
